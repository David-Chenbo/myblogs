#### 1、网络爬虫引发的问题

想必各位心里也清楚，爬虫固然很方便，但是也会引发一系列的问题，想必大家也听说过因为爬虫违法犯罪的事，但是只要我们严格按照网络规范，遵守道德法律，我们正确正常使用而不受这些问题的影响。

根据网络爬虫的尺寸，我们可以简单分为以下三类

| 小规模，数量小，爬取速度不敏感Requests库 | 中规模，数据规模较大，爬取速度敏感Scrapy库 | 大规模，搜索引擎，爬取速度关键定制开发 |
| ---------------------------------------- | ------------------------------------------ | -------------------------------------- |
| 爬取网页，玩转网页                       | 爬取网站，爬取系列网站                     | 爬取全网                               |


比如说，有些网站的服务器就可以设置防范爬虫的骚扰，只接受人类本身操作的请求，这是因为爬虫可以利用计算机的性能，1秒内可以访问成千上万甚至数万次的访问请求，给服务器造成了一定的开销压力，有时甚至会带来的法律问题，比如有些新闻数据，用户隐私等。

因此我们总结了网络爬虫所引发的问题分为了三大类：

服务器骚扰问题
网站内容法律风险问题
用户隐私泄露问题
目前互联网上很多公司对网络爬虫的进行了而一定的限制，关于网络爬虫的限制包括：

* 来源审查：判断User-Agent进行限制

检查来访HTTP协议头的User-Agent域，只响应浏览器或已知友好爬虫的访问。

* 发布公告：Robots协议

告知所有爬虫网站的爬取策略，要求爬虫遵守。

下面我们来看一下关于Robots协议内容：

#### 2、Robots协议

Robots Exclusion Standard网络爬虫排除标准

作用：网站告知网络爬虫哪些页面可以抓取，哪些不行。

形式：在网站根目录下的robots.txt文件。

下面我们来看下京东的Robots协议：http://www.jd.com/robots.txt



User-agent：* --将任意的网络爬虫来源将其定义为User-agent

Disallow: /?* --任何爬虫不许访问？开头的的路径

Disallow: /pop/*.html--任何爬虫不许访问pop/*.html

Disallow: /pinpai/*.html?*--任何爬虫不许访问pinpai/*.html

User-agent: EtaoSpider

Disallow: /

User-agent: HuihuiSpider

Disallow: /

User-agent: GwdangSpider

Disallow: /

User-agent: WochachaSpider

Disallow: /

最后的几个表示这四个User-agent爬虫都不允许爬取京东网站页面，如果你有兴趣，还可以查看其它网站的Robots协议，而且这个协议文件一定是放在网站的根目录下的，你还可以查看以下这几个：

百度：http://www.baidu.com/robots.txt

新浪：http://news.sina.com/robots.txt

QQ：http://www.qq.com/robots.txt

QQ新闻：http://news.qq.com/robots.txt

教育部：http://www.moe.edu.com/robots.txt（无robots协议）

当你打开最后一个教育部的robots网页时，发现出错了，其实是没有，这也就表明允许网络爬虫取爬取信息。

#### 3、Robots协议的遵守方式

* Robots协议的使用：网络爬虫应当能自动或人工识别robots.txt，再进行内容爬取。

* 约束性：Robots协议是建议而非约束性，网络爬虫可以不遵守，但存在法律风险。
  对Robots协议的理解：

 对Robots协议的理解： 

| 访问量很小：可以遵守 | 非商业且偶尔：建议遵守 | 必须遵守 |
| -------------------- | ---------------------- | -------- |
| 访问量较大：建议遵守 | 商业利益：必须遵守     | /        |
| 爬取网页 爬取网页    | 爬取网站 爬取系列网站  | 爬取全网 |




```python

```
